{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<details>\n",
    "<summary>Details :</summary>\n",
    "\n",
    "Name : Sumit Shamlal Chaure\n",
    "\n",
    "Batch : 10\n",
    "\n",
    "Program : Data Science with Python By SkillAcademy\n",
    "\n",
    "Assignment : Capstone Project - Used Car Prediction & Deployment Using Streamlit\n",
    "\n",
    "Topics : **Complete Data Science Project Lifecycle -** Data analytics, EDA, Model Building, Model Evaluation-Training-Testing of Model on sample Data, Model Deployment, Story-Telling & Report Deck.\n",
    "\n",
    "> File Downloads :\n",
    "\n",
    " - [Assignment PDF](https://drive.google.com/file/d/12A-qzEXI_jzGuitXJKuPsIzvpXEWt8MI/view?usp=sharing)\n",
    "\n",
    " - [CSV Dataset](https://drive.google.com/uc?export=download&id=17KxRuOK4uONRZyfYNgjoX62y2yO5KpLY)\n",
    "\n",
    " - [Dataset Alternate Link](https://gist.github.com/Sumit-SC/7a0db90fe24c0e0d08d8bf750725d32e)\n",
    "\n",
    "\n",
    "> My Reports & Files :\n",
    "\n",
    "- [Drive Folder](https://drive.google.com/drive/folders/1N5m1mmKMZH1Fi7y2ZDdf6uMt88uW8CtS?usp=drive_link)\n",
    "\n",
    "- [Report Pdf](https://drive.google.com/file/d/160omB-7Dn0wbcLvhi9pqknx8q6lyfRcQ/view?usp=drive_link)\n",
    "\n",
    "- [Github Repo](https://github.com/Sumit-SC/Data-Science-Capstone-Project)\n",
    "  \n",
    "-  [Streamlit WebApp]()\n",
    "\n",
    "*Note :* Certain markdown linkings like page links wont work on google colab/Jupyter but the same on github or vs-code would take you to respective breakpoints as they have advanced markdown support for inline tagging and MD linkings.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project (Complete  DS/ML lifecycle)\n",
    "\n",
    "### Part 2 - Model Building,Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Steps Involved in Machine Learning Projects (For Part 2 - Model Buidling)</summary>\n",
    "\n",
    "~~1. Understanding the Problem Statement.~~\n",
    "\n",
    "~~2. Data Collection (From Sources/API/Files).~~\n",
    "\n",
    "~3. Data Checking for analysis.~~\n",
    "\n",
    "~~4. Exploratory Data Analysis (To get insights of dataset & problem)~~\n",
    "\n",
    "~~5. Data Pre-Processing.~~\n",
    "\n",
    "6. Model Selection & evaluation.\n",
    "\n",
    "7. Model Training.\n",
    "\n",
    "8. Choosing the Best Model for Best results.\n",
    "\n",
    "9.  Testing with new data & checking the factors such as recall, accuracy & precision.\n",
    "\n",
    "10. Model Deployment\n",
    "\n",
    "11. User testing & benchmarking etc.\n",
    "\n",
    "12. Reiterating the steps with new data and building more accurate models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding the Problem & Dataset\n",
    "\n",
    "Perform EDA and derive Insights from the CAR DETAILS\n",
    "dataset using Various Data Analysis and Data Visualization\n",
    "libraries of Python such as Pandas, Matplotlib & Seaborn.\n",
    "Create and Deploy a ML Model Which can be accessed by all,using Streamlit and GitHub.\n",
    "\n",
    "####\n",
    "<details>\n",
    "<summary>About Dataset</summary>\n",
    "\n",
    "This dataset contains information about used cars.\n",
    "This data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.\n",
    "\n",
    "The columns in the given dataset are as follows:\n",
    "\n",
    "1. name\n",
    "2. year\n",
    "3. selling_price\n",
    "4. km_driven\n",
    "5. fuel\n",
    "6. seller_type\n",
    "7. transmission\n",
    "8. Owner\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Questions\n",
    "</summary>\n",
    "\n",
    "1. Explore the Data using Excel . understand the data and prepare a\n",
    "short summary about the dataset in the PPT.\n",
    "2. Download the CAR DETAILS dataset and perform Data cleaning\n",
    "and Data Pre-Processing if Necessary.\n",
    "3. Use the various methods such as Handling null values, One-Hot\n",
    "Encoding, Imputation and Scaling of Data Pre-Processing where\n",
    "necessary.\n",
    "4. Perform Exploratory data analysis (EDA) on the Data and perform\n",
    "Graphical Analysis on the Data. Include the graphs with\n",
    "conclusions from the Graphical Analysis.\n",
    "5. Prepare the Data for Machine Learning modeling.\n",
    "6. Apply various Machine Learning techniques such as Regression or\n",
    "classification ,Bagging, Ensemble techniques and find out the\n",
    "best model using various Machine Learning model evaluation\n",
    "metrics.\n",
    "7. Save the best model and Load the model.\n",
    "8. Take the original data set and make another dataset by randomly\n",
    "picking 20 data points from the CAR DETAILS dataset and apply\n",
    "the saved model on the same Dataset and test the model.\n",
    "9. Make a GitHub Account by visiting the GitHub Website. Create a\n",
    "repository named Data Science Capstone Project and upload the\n",
    "model with the dataset, code file.\n",
    "10. Create a Streamlit Account by visiting the Streamlit Website.\n",
    "Connect your GitHub account with streamlit.\n",
    "11. Create an app.py file and other dependencies files for Streamlit\n",
    "app to be deployed on Streamlit Cloud. Make a simple website\n",
    "and deploy your ML model on Streamlit, Make the website public.\n",
    "12. Share the Streamlit website and GitHub repository links in the\n",
    "Project PPT.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # for data cleaning and data pre-processing, CSV file I/O,etc\n",
    "import numpy as np              # linear algebra & for mathematical computation\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns           # for visualization\n",
    "from collections import Counter # to count occurrences\n",
    "from tabulate import tabulate   # to make tables for results\n",
    "\n",
    "import warnings                 # for warning removals in code output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scalers & Encoders\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Metrics\n",
    "from sklearn.metrics import (mean_squared_error, r2_score)\n",
    "# Model Libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import pickle           #to save and load model files as pkl file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Processed Dataset (With error handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to upload the dataset directly (Since on Google Colab it will be lost on re-run) - uncomment the below 2 line code and run\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "file_path = \"../data/processed/Processed CAR DETAILS.csv\"\n",
    "file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "try:\n",
    "    # Reading the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Store the filename as an attribute in the DataFrame\n",
    "    df.file_name = file_name\n",
    "    print(f\"\\n '{df.file_name}' loaded successfully.\")\n",
    "\n",
    "# Exception to check if the file has some error like no file at the path, etc.\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_name}' not found at the specified location {\n",
    "          file_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Machine Learning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Dataset (Q3 Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_col = df.select_dtypes(include=\"object\").columns\n",
    "print(f\"Categorical Columns in {df.file_name} :\\n\", category_col)\n",
    "\n",
    "numerical_col = df.select_dtypes(include=\"number\").columns\n",
    "print(f\"\\nNumerical Columns in {df.file_name} :\\n\", numerical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "df_one = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder to encode categorical data\n",
    "label_encoder = LabelEncoder()  # instance of encoder\n",
    "# Loop to  encode data in df\n",
    "for feature in category_col:\n",
    "    df_one[feature] = label_encoder.fit_transform(df[feature])\n",
    "df_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_one.corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix for Used Car Dataset(Encoded)\", fontsize=16)\n",
    "plt.savefig(\"../src/visualization/Correlation Graph - After Label Encoding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = df_one.to_csv(\"../data/processed/cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependant (y) & Independent (x) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropping dependant feature from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_one.drop([\"Selling_Price\"], axis=1)\n",
    "y = df_one[\"Selling_Price\"]\n",
    "\n",
    "print(type(x))\n",
    "print(type(y))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Splitting The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=42\n",
    ")\n",
    "print(f\"Split Check Test values : {3194 * 0.3} & Train values : {3194 * 0.7}\")\n",
    "# rows , columns\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Standarizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit_transform(X_train)\n",
    "sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Model Selection - Evaluation (testing,scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    RandomForestRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "]\n",
    "\n",
    "# Define a list of model names\n",
    "model_names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Ridge Regression\",\n",
    "    \"Lasso Regression\",\n",
    "    \"Random Forest\",\n",
    "    \"k-Nearest Neighbors\",\n",
    "    \"Decision Tree\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"Ada Boost\",\n",
    "]\n",
    "\n",
    "# Initialize variables to keep track of the best model\n",
    "best_model_name = None\n",
    "best_r2_score = -float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "all_results = []\n",
    "\n",
    "# Function to evaluate regression models\n",
    "\n",
    "\n",
    "def evaluate_regression_model(model, model_name, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Mean Squared Error and R-squared Score\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Display results in tabular format\n",
    "    results_table = [\n",
    "        [\"Model\", model_name],\n",
    "        [\"Mean Squared Error\", mse],\n",
    "        [\"R-squared Score\", r2],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(results_table, headers=[\n",
    "          \"Metric\", \"Value\"], tablefmt=\"heavy_grid\"))\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"R-squared Score\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# Iterate over the models\n",
    "for model, model_name in zip(models, model_names):\n",
    "    print(f\"\\n{('-' * 40)}\\n{model_name}\\n{('-' * 40)}\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate and store results for regression model\n",
    "    results = evaluate_regression_model(model, model_name, X_test, y_test)\n",
    "    all_results.append(results)\n",
    "\n",
    "    # Update the best model if needed\n",
    "    if results[\"R-squared Score\"] > best_r2_score:\n",
    "        best_r2_score = results[\"R-squared Score\"]\n",
    "        best_model_name = model_name\n",
    "        best_model = model\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sorting the dataframe by 'R-squared Score' in descending order\n",
    "sorted_results_df = results_df.sort_values(\n",
    "    by=\"R-squared Score\", ascending=False)\n",
    "\n",
    "# Displaying the sorted dataframe\n",
    "print(\"\\nSorted Models by R-squared Score:\")\n",
    "print(sorted_results_df)\n",
    "\n",
    "# Print the best model based on R-squared score\n",
    "print(f\"\\nBest Model based on R-squared Score: {best_model_name}\")\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance table (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = pd.DataFrame(results_df)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Save the best model and Load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Selection & saving as final model (Manual Hard-codeing method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually coding the best model name and changing the below parameter\n",
    "# f_modelname = \"Logistic Regression\"\n",
    "# final_model = LogisticRegression(max_iter=10000, C=1.0, solver=\"lbfgs\")\n",
    "# final_model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the best model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaulate the name and relevant parameter to take for best fit model fitting\n",
    "models_dict = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0, solver=\"lbfgs\"),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5, weights=\"uniform\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "    ),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, learning_rate=1.0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3\n",
    "    ),\n",
    "}\n",
    "# Retrieve the value using the key\n",
    "retrieved_value = models_dict.get(best_model_name)\n",
    "\n",
    "if retrieved_value is not None:\n",
    "    selected_model_name = best_model_name\n",
    "    selected_model = best_model\n",
    "    # selected_model_params = retrieved_value.get_params()\n",
    "    print(f\"Best Model Name: {selected_model_name}\")\n",
    "    print(f\"\\nRetrieved Model Instance: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the best model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating the above hardcoded values by using the above dictonary and for loop\n",
    "f_modelname = selected_model_name\n",
    "f_model = selected_model\n",
    "print(f\"Best Selected Model name : '{\n",
    "      f_modelname}' & \\nits parameters :\\n{f_model.get_params()}\")\n",
    "final_model = f_model\n",
    "final_model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model as pickle file and dumping it to use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb - writing binary\n",
    "pickle.dump(final_model, open(f\"../models/{f_modelname}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = pickle.load(\n",
    "    open(f\"../models/{f_modelname}.pkl\", \"rb\"))  # rb = read binary\n",
    "print(f\"Name of loaded Model : {f_modelname}\")\n",
    "load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the loaded model and predicting on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the imported model\n",
    "print(\"Length of test data: \", len(load_model.predict(X_test)))\n",
    "load_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Take the original data set and make another dataset by randomly picking 20 data points from the CAR DETAILS dataset and apply the saved model on the same Dataset and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating sample data from cleaned df to test on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_datasample = df_one.sample(20)\n",
    "random_datasample_df = random_datasample.drop(\"Selling_Price\", axis=1)\n",
    "print(random_datasample_df.shape)\n",
    "random_datasample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting the index as the randomly generated data has no continuos index (wil delete later,just for understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_datasample_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the random sample dataset and removing the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_datasample_df.to_csv(\n",
    "    \"../data/interim/20_random_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the sample data and checking basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsample_df = pd.read_csv(\"../data/interim/20_random_sample.csv\")\n",
    "print(\n",
    "    \"Shape of loaded sample dataframe:\",\n",
    "    testsample_df.shape,\n",
    "    \"\\n\\nSample Dataframe contents\",\n",
    ")\n",
    "testsample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions on sample dataset against the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making prediction on random data\n",
    "predicted_data = load_model.predict(testsample_df)\n",
    "print(f\"The predicted data from {f_modelname} model:\\n\", predicted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision of Actual and Predicted values by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare the actual data and predicted data\n",
    "prediction_data = random_datasample.copy()\n",
    "prediction_data[\"predicted_target\"] = predicted_data\n",
    "\n",
    "# Calculate the absolute percentage difference\n",
    "prediction_data[\"percentage_difference\"] = abs(\n",
    "    (random_datasample[\"Selling_Price\"] - predicted_data) /\n",
    "    random_datasample[\"Selling_Price\"]\n",
    ") * 100\n",
    "\n",
    "# Print the actual and predicted data\n",
    "print(f\"Actual Data and Predicted Data Comparison based on {\n",
    "      f_modelname} model:\\n\")\n",
    "\n",
    "# Display the results where the absolute percentage difference is less than or equal to 20%\n",
    "safe_predictions = prediction_data[prediction_data[\"percentage_difference\"] <= 20]\n",
    "\n",
    "# Print the safe predictions\n",
    "print(\"Safe Predictions:\")\n",
    "print(safe_predictions[[\"Selling_Price\",\n",
    "      \"predicted_target\", \"percentage_difference\"]])\n",
    "\n",
    "# Print the count and percentage of safe predictions\n",
    "safe_percentage = (len(safe_predictions) / len(prediction_data)) * 100\n",
    "print(f\"\\nPercentage of Safe Predictions: {safe_percentage:.2f}%\")\n",
    "\n",
    "if safe_percentage >= 90:\n",
    "    print(\n",
    "        f\"\\nOur model based on '{f_modelname}' is well trained, with {\n",
    "            safe_percentage:.2f}% safe predictions.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Our model based on '{f_modelname}' needs more training to improve safety, currently at {\n",
    "            safe_percentage:.2f}% safe predictions.\"\n",
    "    )\n",
    "# Save the results as a DataFrame\n",
    "final_results_df = prediction_data[[\n",
    "    \"Selling_Price\", \"predicted_target\", \"percentage_difference\"]]\n",
    "\n",
    "final_results_df.to_csv('../reports/model_predicted_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra** - Saving the output in a csv file and also inside a text file for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the final results in a output text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file path for saving results\n",
    "output_file_path = '../reports/final_results.txt'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_file_path, 'w') as f:\n",
    "    # Write the DataFrame to the file\n",
    "    f.write(final_results_df.to_string())\n",
    "\n",
    "    # Add a separator and header for additional information\n",
    "    f.write('\\n\\n---------------------------------------\\n')\n",
    "    f.write(f\"Printing the results of our {\n",
    "            f_modelname} prediction on random 20 data samples.\\n\")\n",
    "\n",
    "    # Number of correct predictions\n",
    "    correct_predictions = sum(\n",
    "        final_results_df['Selling_Price'] == final_results_df['predicted_target']\n",
    "    )\n",
    "\n",
    "    # Total predictions\n",
    "    total_predictions = len(final_results_df)\n",
    "\n",
    "    # Calculate the percentage of correct predictions\n",
    "    percentage_correct_predictions = (\n",
    "        correct_predictions / total_predictions) * 100\n",
    "\n",
    "    f.write(f\"Number of correct predictions: {correct_predictions}\\n\")\n",
    "    f.write(f\"Percentage of correct predictions: {\n",
    "            percentage_correct_predictions:.2f}%\\n\")\n",
    "\n",
    "    # Check if the absolute percentage difference is less than or equal to 20%\n",
    "    safe_predictions = final_results_df[final_results_df[\"percentage_difference\"] <= 20]\n",
    "    safe_percentage = (len(safe_predictions) / total_predictions) * 100\n",
    "\n",
    "    f.write(f\"\\nNumber of safe predictions (Absolute Percentage Difference <= 20%): {\n",
    "            len(safe_predictions)}\\n\")\n",
    "    f.write(f\"Percentage of safe predictions: {safe_percentage:.2f}%\\n\")\n",
    "\n",
    "    # Print the result in the output file\n",
    "    if safe_percentage >= 80:\n",
    "        f.write(f\"\\nOur model based on '{f_modelname}' is well trained, with {\n",
    "                safe_percentage:.2f}% safe predictions.\\n\")\n",
    "    else:\n",
    "        f.write(f\"Our model based on '{f_modelname}' needs more training to improve safety, currently at {\n",
    "                safe_percentage:.2f}% safe predictions.\\n\")\n",
    "\n",
    "# Print the confirmation message\n",
    "print(f\"Results saved in {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
